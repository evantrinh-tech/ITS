# H∆Ø·ªöNG D·∫™N V·∫¨N H√ÄNH CODE H·ªÜ TH·ªêNG ITS
## Intelligent Transportation System - Ph√°t hi·ªán S·ª± c·ªë Giao th√¥ng

**Phi√™n b·∫£n**: 1.0  
**Ng√†y c·∫≠p nh·∫≠t**: 15/01/2026  
**M·ª•c ƒë√≠ch**: Gi·∫£i th√≠ch c·∫•u tr√∫c code, c√°ch v·∫≠n h√†nh c√°c file/folder trong h·ªá th·ªëng

---

## üìÇ 1. T·ªîNG QUAN C·∫§U TR√öC TH·ª¶ M·ª§C

```
ITS/
‚îú‚îÄ‚îÄ üìÅ src/                      # Source code ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/               # C√°c m√¥ h√¨nh ML/DL
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data_processing/      # X·ª≠ l√Ω d·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ serving/              # API v√† serving
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ training/             # Training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ database/             # Database models
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                # Utilities
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                     # D·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ images/               # Dataset ·∫£nh
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ processed/            # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ raw/                  # D·ªØ li·ªáu th√¥
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                   # M√¥ h√¨nh ƒë√£ train
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ CNN_model/
‚îÇ       ‚îî‚îÄ‚îÄ model.keras
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/                     # T√†i li·ªáu
‚îú‚îÄ‚îÄ üìÅ tests/                    # Unit tests
‚îú‚îÄ‚îÄ üìÅ configs/                  # Configuration files
‚îú‚îÄ‚îÄ üìÅ pipelines/                # Training pipelines
‚îú‚îÄ‚îÄ üìÅ scripts/                  # Utility scripts
‚îú‚îÄ‚îÄ üìÅ logs/                     # Log files
‚îÇ
‚îú‚îÄ‚îÄ üìÑ app.py                    # Streamlit dashboard (Entry point ch√≠nh)
‚îú‚îÄ‚îÄ üìÑ train_cnn.py              # Training script
‚îú‚îÄ‚îÄ üìÑ start_api.py              # API server entry point
‚îú‚îÄ‚îÄ üìÑ test_cnn_image.py         # Test v·ªõi ·∫£nh
‚îú‚îÄ‚îÄ üìÑ test_cnn_video.py         # Test v·ªõi video
‚îú‚îÄ‚îÄ üìÑ test_api.py               # Test API
‚îú‚îÄ‚îÄ üìÑ run_streamlit.py          # Helper ch·∫°y Streamlit
‚îú‚îÄ‚îÄ üìÑ he_thong.bat              # Menu h·ªá th·ªëng (Windows)
‚îî‚îÄ‚îÄ üìÑ requirements.txt          # Dependencies
```

---

## üì¶ 2. TH∆Ø M·ª§C `src/` - SOURCE CODE CH√çNH

### 2.1. `src/models/` - C√°c M√¥ h√¨nh Machine Learning

#### üìÑ `base_model.py`
**Ch·ª©c nƒÉng**: Base class cho t·∫•t c·∫£ c√°c models

**N·ªôi dung ch√≠nh**:
```python
class BaseModel:
    def __init__(self, model_type: str, config: Optional[Dict] = None)
    def build(self, input_shape: Tuple[int, ...], **kwargs) -> None
    def train(self, X_train, y_train, X_val, y_val, **kwargs) -> Dict
    def predict(self, X: np.ndarray) -> np.ndarray
    def save(self, path: Path) -> None
    def load(self, path: Path) -> None
```

**Khi n√†o d√πng**: Khi t·∫°o model m·ªõi, k·∫ø th·ª´a class n√†y

---

#### üìÑ `cnn.py` - CNN Model (QUAN TR·ªåNG NH·∫§T)
**Ch·ª©c nƒÉng**: Convolutional Neural Network v·ªõi Transfer Learning

**Class ch√≠nh**: `CNNModel`

**Parameters quan tr·ªçng**:
- `use_transfer_learning`: True/False (m·∫∑c ƒë·ªãnh True)
- `base_model`: 'MobileNetV2' | 'ResNet50' | 'VGG16'
- `image_size`: (224, 224) ho·∫∑c (128, 128)
- `learning_rate`: 0.001 (m·∫∑c ƒë·ªãnh)

**C√°ch s·ª≠ d·ª•ng**:
```python
from src.models.cnn import CNNModel

# T·∫°o model
model = CNNModel(
    use_transfer_learning=True,
    base_model='MobileNetV2',
    image_size=(224, 224),
    learning_rate=0.001
)

# Build model
model.build(input_shape=(224, 224, 3))

# Train
history = model.train(
    X_train, y_train,
    X_val, y_val,
    epochs=50,
    batch_size=32
)

# Predict
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)

# Save
model.save('models/CNN_model/')

# Load
model.load('models/CNN_model/model.keras')
```

**Transfer Learning Flow**:
1. Load pre-trained base model (ImageNet weights)
2. Freeze base layers
3. Add custom top layers (Dense, Dropout)
4. Train top layers
5. Unfreeze base layers v√† fine-tune

**Khi n√†o d√πng**: ƒê√¢y l√† model ch√≠nh c·ªßa h·ªá th·ªëng, d√πng cho detection

---

#### üìÑ `ann.py` - Artificial Neural Network
**Ch·ª©c nƒÉng**: Feed-forward Neural Network ƒë∆°n gi·∫£n

**Class**: `ANNModel`

**C√°ch d√πng**: T∆∞∆°ng t·ª± CNN, nh∆∞ng nh·∫≠n input l√† features vector thay v√¨ ·∫£nh

---

#### üìÑ `rnn.py` - Recurrent Neural Network
**Ch·ª©c nƒÉng**: LSTM/GRU cho temporal patterns

**Class**: `RNNModel`

**Khi n√†o d√πng**: Khi c·∫ßn ph√¢n t√≠ch time-series data (v√≠ d·ª•: sensor data theo th·ªùi gian)

---

#### üìÑ `rbfnn.py` - Radial Basis Function Neural Network
**Ch·ª©c nƒÉng**: RBFNN cho classification

**Class**: `RBFNNModel`

**Khi n√†o d√πng**: Alternative approach, √≠t d√πng trong project n√†y

---

#### üìÑ `segmentation.py` - U-Net Segmentation
**Ch·ª©c nƒÉng**: U-Net architecture cho pixel-level segmentation

**Status**: ƒê√£ thi·∫øt k·∫ø ki·∫øn tr√∫c, ch∆∞a implement ho√†n ch·ªânh

**Khi n√†o d√πng**: Khi c·∫ßn ph√¢n v√πng (segmentation) v√πng s·ª± c·ªë trong ·∫£nh

---

### 2.2. `src/data_processing/` - X·ª≠ l√Ω D·ªØ li·ªáu

#### üìÑ `image_processor.py`
**Ch·ª©c nƒÉng**: X·ª≠ l√Ω ·∫£nh (resize, normalize, augmentation)

**Class**: `ImageProcessor`

**Methods ch√≠nh**:
```python
def load_image(image_path: str, target_size=(224, 224)) -> np.ndarray
def preprocess_image(img: np.ndarray) -> np.ndarray
def normalize(img: np.ndarray) -> np.ndarray
def augment(img: np.ndarray) -> np.ndarray
```

**C√°ch d√πng**:
```python
from src.data_processing.image_processor import ImageProcessor

processor = ImageProcessor()
img = processor.load_image('path/to/image.jpg', target_size=(224, 224))
img_processed = processor.preprocess_image(img)
```

---

#### üìÑ `preprocessors.py`
**Ch·ª©c nƒÉng**: Preprocessing t·ªïng qu√°t cho data

**Functions**:
- `load_dataset(data_dir)`: Load ·∫£nh t·ª´ th∆∞ m·ª•c
- `split_data(X, y, test_size)`: Split train/validation
- `create_data_generator()`: T·∫°o data generator cho training

**C√°ch d√πng**:
```python
from src.data_processing.preprocessors import load_dataset, split_data

# Load dataset
X, y, class_names = load_dataset('data/images/')

# Split
X_train, X_val, y_train, y_val = split_data(X, y, test_size=0.2)
```

---

#### üìÑ `collectors.py`
**Ch·ª©c nƒÉng**: Thu th·∫≠p d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn

**Khi n√†o d√πng**: Khi c·∫ßn crawl/collect th√™m data

---

#### üìÑ `validators.py`
**Ch·ª©c nƒÉng**: Validate ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu

**Functions**:
- `validate_image(img_path)`: Ki·ªÉm tra ·∫£nh c√≥ h·ª£p l·ªá
- `check_dataset_balance(y)`: Ki·ªÉm tra dataset c√≥ balanced
- `detect_duplicates(X)`: T√¨m ·∫£nh tr√πng l·∫∑p

---

### 2.3. `src/serving/` - API v√† Serving

#### üìÑ `api.py` - FastAPI Server (QUAN TR·ªåNG)
**Ch·ª©c nƒÉng**: REST API endpoints cho h·ªá th·ªëng

**Endpoints ch√≠nh**:

1. **GET `/`**: Root endpoint
2. **GET `/health`**: Health check
3. **POST `/predict`**: Prediction endpoint
4. **GET `/metrics`**: Monitoring metrics
5. **POST `/model/reload`**: Reload model
6. **GET `/model/info`**: Model information

**C√°ch ch·∫°y**:
```bash
# C√°ch 1: Tr·ª±c ti·∫øp
python start_api.py

# C√°ch 2: Qua uvicorn
uvicorn src.serving.api:app --host 0.0.0.0 --port 8000

# C√°ch 3: Qua menu
he_thong.bat -> [2] Ch·∫°y API Server
```

**Swagger docs**: http://localhost:8000/docs

**C√°ch test**:
```bash
# Health check
curl http://localhost:8000/health

# Predict
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"data": [{"timestamp": "2024-01-15", "detector_id": "001", "volume": 100, "speed": 60, "occupancy": 0.5}]}'
```

---

#### üìÑ `predictor.py` - Prediction Logic
**Ch·ª©c nƒÉng**: Load model v√† th·ª±c hi·ªán prediction

**Class**: `ModelPredictor`

**Methods**:
```python
def load_model(model_path: Path)
def predict(data: pd.DataFrame) -> List[Dict]
def predict_image(image_path: str) -> Dict
def is_model_loaded() -> bool
def get_model_version() -> str
```

**C√°ch d√πng**:
```python
from src.serving.predictor import ModelPredictor

predictor = ModelPredictor()
predictor.load_model('models/CNN_model/model.keras')

result = predictor.predict_image('path/to/image.jpg')
print(result)  # {'prediction': 'incident', 'probability': 0.92}
```

---

#### üìÑ `temporal_confirmation.py` - Temporal Confirmation
**Ch·ª©c nƒÉng**: Gi·∫£m false alarms b·∫±ng x√°c nh·∫≠n qua nhi·ªÅu frames

**Class**: `TemporalConfirmation`

**Parameters**:
- `k_frames`: S·ªë frames ƒë·ªÉ x√°c nh·∫≠n (m·∫∑c ƒë·ªãnh 5)
- `threshold`: Threshold probability (m·∫∑c ƒë·ªãnh 0.7)
- `cooldown`: Cooldown period (m·∫∑c ƒë·ªãnh 30 frames)

**Algorithm**:
```python
# Pseudo-code
if moving_average(probabilities[-k_frames:]) > threshold:
    if not in_cooldown:
        trigger_incident()
        start_cooldown()
```

**C√°ch d√πng**:
```python
from src.serving.temporal_confirmation import TemporalConfirmation

tc = TemporalConfirmation(k_frames=5, threshold=0.7)

for frame in video_frames:
    prob = model.predict(frame)
    incident = tc.process(prob)
    if incident:
        print(f"INCIDENT CONFIRMED at frame {frame_number}")
```

---

#### üìÑ `monitoring.py`
**Ch·ª©c nƒÉng**: System monitoring v√† metrics collection

**Class**: `MetricsCollector`

---

### 2.4. `src/training/` - Training Pipeline

#### üìÑ `trainer.py`
**Ch·ª©c nƒÉng**: Training logic v√† pipeline

**Class**: `Trainer`

**Methods**:
- `train_model()`: Main training function
- `setup_callbacks()`: Setup callbacks (EarlyStopping, etc.)
- `log_metrics()`: Log to MLflow

---

#### üìÑ `evaluator.py`
**Ch·ª©c nƒÉng**: Model evaluation

**Functions**:
- `evaluate_model(model, X_test, y_test)`: Evaluate v√† t√≠nh metrics
- `calculate_metrics(y_true, y_pred)`: Calculate Precision, Recall, F1
- `generate_confusion_matrix()`: T·∫°o confusion matrix

---

#### üìÑ `visualizer.py`
**Ch·ª©c nƒÉng**: Visualization cho training

**Functions**:
- `plot_training_history()`: Plot loss v√† accuracy curves
- `plot_confusion_matrix()`: V·∫Ω confusion matrix
- `plot_roc_curve()`: V·∫Ω ROC curve

---

### 2.5. `src/database/` - Database

#### üìÑ `models.py` - SQLAlchemy Models
**Ch·ª©c nƒÉng**: Database schema definitions

**Models ch√≠nh**:

```python
class Incident(Base):
    __tablename__ = 'incidents'
    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime)
    camera_id = Column(String)
    confidence_score = Column(Float)
    status = Column(String)  # detected, confirmed, false_alarm
    image_path = Column(String)

class Prediction(Base):
    __tablename__ = 'predictions'
    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime)
    prediction = Column(String)
    probability = Column(Float)
    model_version = Column(String)
```

**C√°ch d√πng**:
```python
from src.database.models import Incident, Prediction
from src.utils.config import get_db_session

session = get_db_session()

# Create incident
incident = Incident(
    timestamp=datetime.now(),
    camera_id='CAM001',
    confidence_score=0.92,
    status='detected'
)
session.add(incident)
session.commit()

# Query
incidents = session.query(Incident).filter(
    Incident.status == 'confirmed'
).all()
```

---

### 2.6. `src/utils/` - Utilities

#### üìÑ `config.py`
**Ch·ª©c nƒÉng**: Configuration management

**Settings**:
```python
class Settings:
    # API settings
    api_host = "0.0.0.0"
    api_port = 8000
    api_workers = 4
    
    # Model paths
    model_dir = Path("models/")
    default_model_path = model_dir / "CNN_model" / "model.keras"
    
    # Database
    database_url = "postgresql://user:pass@localhost/traffic_db"
```

---

#### üìÑ `logger.py`
**Ch·ª©c nƒÉng**: Logging configuration

**C√°ch d√πng**:
```python
from src.utils.logger import logger

logger.info("Model training started")
logger.warning("Low confidence prediction")
logger.error("Failed to load model")
```

---

## üìÑ 3. ROOT LEVEL FILES - ENTRY POINTS

### üìÑ `app.py` - Streamlit Dashboard (‚≠ê ENTRY POINT CH√çNH)

**Ch·ª©c nƒÉng**: Giao di·ªán web dashboard t∆∞∆°ng t√°c

**C√°c Tab**:
1. ** Trang ch·ªß**: Overview h·ªá th·ªëng
2. ** Test m√¥ h√¨nh**: Upload ·∫£nh/video, predict
3. ** Hu·∫•n luy·ªán m√¥ h√¨nh**: Training interface
4. ** Xem k·∫øt qu·∫£**: Metrics visualization
5. **üö® Qu·∫£n l√Ω Incidents**: Incident management

**C√°ch ch·∫°y**:
```bash
# C√°ch 1
python run_streamlit.py

# C√°ch 2
streamlit run app.py

# C√°ch 3
he_thong.bat -> [1] Giao di·ªán Web
```

**URL**: http://localhost:8501

**C·∫•u tr√∫c code**:
```python
import streamlit as st
from src.models.cnn import CNNModel
from src.data_processing.preprocessors import load_dataset

st.title(" H·ªá th·ªëng Ph√°t hi·ªán S·ª± c·ªë Giao th√¥ng")

# Sidebar
page = st.sidebar.selectbox("Ch·ªçn trang", ["Trang ch·ªß", "Test", "Hu·∫•n luy·ªán"])

if page == "Test":
    uploaded_file = st.file_uploader("Upload ·∫£nh")
    if uploaded_file:
        # Process and predict
        result = predict_image(uploaded_file)
        st.write(f"K·∫øt qu·∫£: {result}")

elif page == "Hu·∫•n luy·ªán":
    epochs = st.slider("Epochs", 10, 100, 50)
    batch_size = st.selectbox("Batch size", [16, 32, 64])
    
    if st.button("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán"):
        # Training logic
        train_model(epochs, batch_size)
```

---

### üìÑ `train_cnn.py` - Training Script

**Ch·ª©c nƒÉng**: Script ƒë·ªÉ train CNN model t·ª´ command line

**C√°ch ch·∫°y**:
```bash
# Basic
python train_cnn.py

# V·ªõi arguments (n·∫øu c√≥)
python train_cnn.py --epochs 50 --batch_size 32 --image_size 224

# Qua menu
he_thong.bat -> [3] Hu·∫•n luy·ªán m√¥ h√¨nh -> [1] CNN
```

**Flow**:
1. Load dataset t·ª´ `data/images/`
2. Split train/validation
3. Create CNN model
4. Train v·ªõi data augmentation
5. Evaluate
6. Save model to `models/CNN_model/`
7. Log metrics to MLflow

**Output**:
- Model file: `models/CNN_model/model.keras`
- Training history: `logs/training_history.json`
- Plots: `logs/plots/`

---

### üìÑ `start_api.py` - API Server Entry Point

**Ch·ª©c nƒÉng**: Start FastAPI server

**C√°ch ch·∫°y**:
```bash
python start_api.py

# Ho·∫∑c
he_thong.bat -> [2] Ch·∫°y API Server
```

**Code**:
```python
from src.serving.api import app, main

if __name__ == '__main__':
    main()  # Load model v√† start server
```

---

### üìÑ `test_cnn_image.py` - Test v·ªõi ·∫¢nh

**Ch·ª©c nƒÉng**: Test model v·ªõi single image

**C√°ch ch·∫°y**:
```bash
python test_cnn_image.py path/to/image.jpg

# Ho·∫∑c
he_thong.bat -> [4] Test m√¥ h√¨nh -> [1] Test v·ªõi ·∫£nh
```

**Output**:
```
Loading model from models/CNN_model/model.keras...
Processing image: path/to/image.jpg

Results:
  Prediction: INCIDENT
  Probability: 0.92
  Confidence: 92%
```

---

### üìÑ `test_cnn_video.py` - Test v·ªõi Video

**Ch·ª©c nƒÉng**: Test model v·ªõi video file

**C√°ch ch·∫°y**:
```bash
python test_cnn_video.py path/to/video.mp4

# Ho·∫∑c
he_thong.bat -> [4] Test m√¥ h√¨nh -> [2] Test v·ªõi video
```

**Flow**:
1. Load video
2. Extract frames
3. Predict each frame
4. Apply temporal confirmation
5. Generate incident timeline

**Output**:
```
Processing video: path/to/video.mp4
Total frames: 300
Processing... [=====>] 100%

Results:
  Total frames: 300
  Incidents detected: 5
  False alarm rate: 8%
  
Incident Timeline:
  Frame 45-52: INCIDENT (prob=0.92)
  Frame 130-138: INCIDENT (prob=0.87)
  ...
```

---

### üìÑ `test_api.py` - Test API Endpoints

**Ch·ª©c nƒÉng**: Test t·∫•t c·∫£ API endpoints

**C√°ch ch·∫°y**:
```bash
python test_api.py

# Ho·∫∑c
he_thong.bat -> [4] Test m√¥ h√¨nh -> [4] Test API
```

**Tests**:
- Health check endpoint
- Predict endpoint
- Model info endpoint

---

### üìÑ `run_streamlit.py` - Streamlit Helper

**Ch·ª©c nƒÉng**: Helper script ƒë·ªÉ ch·∫°y Streamlit

**Code**:
```python
import os
os.system('streamlit run app.py')
```

---

### üìÑ `he_thong.bat` - Menu H·ªá th·ªëng (Windows)

**Ch·ª©c nƒÉng**: Menu t∆∞∆°ng t√°c ƒë·ªÉ qu·∫£n l√Ω h·ªá th·ªëng

**Options**:
1. Giao di·ªán Web (Streamlit)
2. Ch·∫°y API Server
3. Hu·∫•n luy·ªán m√¥ h√¨nh
4. Test m√¥ h√¨nh
5. Ki·ªÉm tra tr·∫°ng th√°i
6. T·∫°o Virtual Environment
7. Setup Database
8. D·ªçn d·∫πp h·ªá th·ªëng
9. Quick Start

**C√°ch d√πng**:
```bash
he_thong.bat
```

---

##  4. DATA ORGANIZATION

### 4.1. `data/images/` - Dataset

**C·∫•u tr√∫c**:
```
data/images/
‚îú‚îÄ‚îÄ normal/          # ·∫¢nh giao th√¥ng b√¨nh th∆∞·ªùng
‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg
‚îÇ   ‚îú‚îÄ‚îÄ img002.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ incident/        # ·∫¢nh c√≥ s·ª± c·ªë
    ‚îú‚îÄ‚îÄ img001.jpg
    ‚îú‚îÄ‚îÄ img002.jpg
    ‚îî‚îÄ‚îÄ ...
```

**Y√™u c·∫ßu**:
- T·ªëi thi·ªÉu 50 ·∫£nh/class
- Khuy·∫øn ngh·ªã 200+ ·∫£nh/class
- Format: `.jpg`, `.jpeg`, `.png`

**C√°ch add th√™m data**:
1. Copy ·∫£nh v√†o th∆∞ m·ª•c t∆∞∆°ng ·ª©ng (`normal/` ho·∫∑c `incident/`)
2. Ch·∫°y l·∫°i training

---

### 4.2. `models/CNN_model/` - Trained Models

**Files**:
- `model.keras`: Model ƒë√£ train (Keras format)
- `weights.h5`: Model weights (optional)
- `training_history.json`: Training history

**C√°ch load model**:
```python
from tensorflow import keras

model = keras.models.load_model('models/CNN_model/model.keras')
```

---

## üîÑ 5. WORKFLOW TH√îNG D·ª§NG

### 5.1. Workflow Training Model M·ªõi

```bash
# 1. Chu·∫©n b·ªã data
# Copy ·∫£nh v√†o data/images/normal/ v√† data/images/incident/

# 2. Train model
python train_cnn.py
# Ho·∫∑c qua Streamlit: app.py -> Tab "Hu·∫•n luy·ªán"

# 3. Model s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i models/CNN_model/model.keras

# 4. Test model
python test_cnn_image.py data/images/incident/test.jpg

# 5. Deploy model (start API)
python start_api.py
```

---

### 5.2. Workflow Test H·ªá th·ªëng

```bash
# 1. Start Streamlit dashboard
python run_streamlit.py

# 2. M·ªü browser: http://localhost:8501

# 3. Upload ·∫£nh test trong tab "Test m√¥ h√¨nh"

# 4. Xem k·∫øt qu·∫£ prediction
```

---

### 5.3. Workflow S·ª≠ d·ª•ng API

```bash
# 1. Start API server
python start_api.py

# 2. Test health
curl http://localhost:8000/health

# 3. Predict
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d @request.json

# 4. Xem Swagger docs
# Browser: http://localhost:8000/docs
```

---

### 5.4. Workflow Development

```bash
# 1. Activate virtual environment
venv311\Scripts\activate  # Windows
source venv311/bin/activate  # Linux/Mac

# 2. Install dependencies
pip install -r requirements.txt

# 3. Make changes to code

# 4. Test
pytest tests/

# 5. Run application
python app.py  # ho·∫∑c python start_api.py
```

---

## üõ†Ô∏è 6. CONFIGURATION FILES

### üìÑ `requirements.txt`
**Ch·ª©c nƒÉng**: Python dependencies

**C√†i ƒë·∫∑t**:
```bash
pip install -r requirements.txt
```

**Main dependencies**:
- tensorflow
- streamlit
- fastapi
- opencv-python
- sqlalchemy
- mlflow

---

### üìÑ `configs/training_config.yaml`
**Ch·ª©c nƒÉng**: Training configuration

**Content**:
```yaml
training:
  epochs: 50
  batch_size: 32
  image_size: [224, 224]
  learning_rate: 0.001
  base_model: "MobileNetV2"
  validation_split: 0.2
```

**C√°ch d√πng**:
```python
import yaml

with open('configs/training_config.yaml') as f:
    config = yaml.safe_load(f)
    
epochs = config['training']['epochs']
```

---

##  7. IMPORT PATHS - QUAN TR·ªåNG

### Correct Import Examples

```python
# Models
from src.models.cnn import CNNModel
from src.models.ann import ANNModel
from src.models.segmentation import UNetSegmentation

# Data Processing
from src.data_processing.image_processor import ImageProcessor
from src.data_processing.preprocessors import load_dataset, split_data

# Serving
from src.serving.predictor import ModelPredictor
from src.serving.api import app
from src.serving.temporal_confirmation import TemporalConfirmation

# Training
from src.training.trainer import Trainer
from src.training.evaluator import evaluate_model

# Utils
from src.utils.config import settings
from src.utils.logger import logger

# Database
from src.database.models import Incident, Prediction
```

### PYTHONPATH Setup

**Windows**:
```bash
set PYTHONPATH=%PYTHONPATH%;C:\path\to\ITS
```

**Linux/Mac**:
```bash
export PYTHONPATH=$PYTHONPATH:/path/to/ITS
```

**Ho·∫∑c d√πng script**:
```bash
# Windows
set_pythonpath.bat

# Linux/Mac
source set_pythonpath.sh
```

---

## üìñ 8. CODE EXAMPLES

### 8.1. Train Model t·ª´ Code

```python
from src.models.cnn import CNNModel
from src.data_processing.preprocessors import load_dataset, split_data

# 1. Load dataset
X, y, class_names = load_dataset('data/images/')
print(f"Loaded {len(X)} images")
print(f"Classes: {class_names}")

# 2. Split data
X_train, X_val, y_train, y_val = split_data(X, y, test_size=0.2)

# 3. Create model
model = CNNModel(
    use_transfer_learning=True,
    base_model='MobileNetV2',
    image_size=(224, 224),
    learning_rate=0.001
)

# 4. Train
history = model.train(
    X_train, y_train,
    X_val, y_val,
    epochs=50,
    batch_size=32,
    verbose=1
)

# 5. Save
model.save('models/CNN_model/')
print("Model saved!")
```

---

### 8.2. Predict t·ª´ Code

```python
from src.models.cnn import CNNModel
from src.data_processing.image_processor import ImageProcessor

# 1. Load model
model = CNNModel()
model.load('models/CNN_model/model.keras')

# 2. Load v√† preprocess image
processor = ImageProcessor()
img = processor.load_image('path/to/image.jpg', target_size=(224, 224))
img = processor.preprocess_image(img)
img = img.reshape(1, 224, 224, 3)  # Add batch dimension

# 3. Predict
prediction = model.predict(img)[0]
probability = model.predict_proba(img)[0]

# 4. Interpret
class_names = ['normal', 'incident']
result = class_names[prediction]
print(f"Prediction: {result}")
print(f"Probability: {probability:.2%}")
```

---

### 8.3. Process Video

```python
import cv2
from src.models.cnn import CNNModel
from src.serving.temporal_confirmation import TemporalConfirmation

# 1. Load model
model = CNNModel()
model.load('models/CNN_model/model.keras')

# 2. Setup temporal confirmation
tc = TemporalConfirmation(k_frames=5, threshold=0.7)

# 3. Open video
cap = cv2.VideoCapture('path/to/video.mp4')

frame_number = 0
incidents = []

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # Preprocess
    frame_resized = cv2.resize(frame, (224, 224))
    frame_normalized = frame_resized / 255.0
    frame_batch = frame_normalized.reshape(1, 224, 224, 3)
    
    # Predict
    prob = model.predict_proba(frame_batch)[0]
    
    # Temporal confirmation
    incident = tc.process(prob)
    
    if incident:
        incidents.append({
            'frame': frame_number,
            'probability': prob
        })
        print(f"INCIDENT at frame {frame_number}")
    
    frame_number += 1

cap.release()

print(f"\nTotal incidents: {len(incidents)}")
```

---

## üêõ 9. DEBUGGING & TROUBLESHOOTING

### 9.1. Model kh√¥ng load ƒë∆∞·ª£c

**L·ªói**: `FileNotFoundError: Kh√¥ng t√¨m th·∫•y model`

**Gi·∫£i ph√°p**:
```bash
# Ki·ªÉm tra file model c√≥ t·ªìn t·∫°i
ls models/CNN_model/model.keras

# N·∫øu kh√¥ng c√≥, train l·∫°i
python train_cnn.py
```

---

### 9.2. Import Error

**L·ªói**: `ModuleNotFoundError: No module named 'src'`

**Gi·∫£i ph√°p**:
```bash
# Set PYTHONPATH
set PYTHONPATH=%PYTHONPATH%;C:\path\to\ITS  # Windows
export PYTHONPATH=$PYTHONPATH:/path/to/ITS   # Linux

# Ho·∫∑c ch·∫°y t·ª´ root directory
cd ITS/
python -m src.models.cnn
```

---

### 9.3. TensorFlow l·ªói

**L·ªói**: `Could not load dynamic library 'cudart64_110.dll'`

**Gi·∫£i ph√°p**: ƒêang d√πng TensorFlow-GPU nh∆∞ng kh√¥ng c√≥ CUDA. C√†i TensorFlow CPU:
```bash
pip uninstall tensorflow-gpu
pip install tensorflow
```

---

### 9.4. Streamlit kh√¥ng ch·∫°y

**L·ªói**: `streamlit: command not found`

**Gi·∫£i ph√°p**:
```bash
# C√†i l·∫°i streamlit
pip install --upgrade streamlit

# Ho·∫∑c ch·∫°y tr·ª±c ti·∫øp
python -m streamlit run app.py
```

---

## üìö 10. BEST PRACTICES

### 10.1. Code Organization

 **N√äN**:
- ƒê·∫∑t code trong `src/` theo modules
- S·ª≠ d·ª•ng absolute imports (`from src.models.cnn import ...`)
- Docstrings cho t·∫•t c·∫£ functions/classes
- Type hints cho parameters

‚ùå **KH√îNG N√äN**:
- Relative imports (`from ..models import`)
- Hardcode paths
- Code tr·ª±c ti·∫øp trong root files

---

### 10.2. Training

 **N√äN**:
- Save model sau m·ªói epoch t·ªët nh·∫•t (ModelCheckpoint)
- Log metrics v√†o MLflow
- Validate tr√™n validation set
- Early stopping ƒë·ªÉ tr√°nh overfit

‚ùå **KH√îNG N√äN**:
- Train qu√° nhi·ªÅu epochs m√† kh√¥ng EarlyStopping
- Qu√™n validate
- Train tr√™n to√†n b·ªô dataset (kh√¥ng split)

---

### 10.3. Testing

 **N√äN**:
- Test model tr√™n test set ri√™ng
- S·ª≠ d·ª•ng confusion matrix
- Calculate nhi·ªÅu metrics (Precision, Recall, F1)

‚ùå **KH√îNG N√äN**:
- Test tr√™n training data
- Ch·ªâ nh√¨n accuracy

---

## üìû 11. H·ªñ TR·ª¢

### T√†i li·ªáu tham kh·∫£o
- `README.md` - Overview h·ªá th·ªëng
- `BAO_CAO_TIEN_DO_HE_THONG.md` - B√°o c√°o chi ti·∫øt
- `docs/ARCHITECTURE.md` - Ki·∫øn tr√∫c
- `docs/HUONG_DAN_SU_DUNG.md` - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

### Code documentation
- Docstrings trong m·ªói file `.py`
- Comments inline cho logic ph·ª©c t·∫°p
- Type hints

---

**Ch√∫c b·∫°n code hi·ªáu qu·∫£! **

---

*File t·∫°o ng√†y: 15/01/2026*  
*Version: 1.0*  
*C·∫≠p nh·∫≠t: Khi c√≥ thay ƒë·ªïi l·ªõn trong codebase*
